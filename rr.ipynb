{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslationRecognizerCallback open.\n",
      "请您通过麦克风讲话体验实时语音识别和翻译功能\n",
      "request id:  80b35582697244d3a73795574c9ab7da\n",
      "usage:  {}\n",
      "translation_languages:  ['en']\n",
      "sentence id:  0\n",
      "translate to english:  Hello\n",
      "sentence id:  0\n",
      "transcription:  你好。\n",
      "request id:  80b35582697244d3a73795574c9ab7da\n",
      "usage:  {}\n",
      "translation_languages:  ['en']\n",
      "sentence id:  0\n",
      "translate to english:  Hello, world.\n",
      "sentence id:  0\n",
      "transcription:  你好，世界。\n",
      "request id:  80b35582697244d3a73795574c9ab7da\n",
      "usage:  {'duration': 8}\n",
      "translation_languages:  ['en']\n",
      "sentence id:  0\n",
      "translate to english:  Hello, world.\n",
      "sentence id:  0\n",
      "transcription:  你好，世界。\n",
      "request id:  215a230d7f7b4f099555efd41744f1ee\n",
      "usage:  {'duration': 21}\n",
      "translation_languages:  ['en']\n",
      "sentence id:  1\n",
      "translate to english:  How do I put it on the board?\n",
      "sentence id:  1\n",
      "transcription:  我怎么上10个呀，来一下。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m---> 78\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m         translator\u001b[38;5;241m.\u001b[39msend_audio_frame(data)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Anaconda\\envs\\agent\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For prerequisites running the following sample, visit https://help.aliyun.com/document_detail/xxxxx.html\n",
    "\n",
    "import pyaudio\n",
    "import dashscope\n",
    "from dashscope.audio.asr import *\n",
    "dashscope.api_key = \"sk-615616fb539749dda57c80cc0928669d\"\n",
    "\n",
    "# 若没有将API Key配置到环境变量中，需将your-api-key替换为自己的API Key\n",
    "\n",
    "mic = None\n",
    "stream = None\n",
    "\n",
    "class Callback(TranslationRecognizerCallback):\n",
    "    def on_open(self) -> None:\n",
    "        global mic\n",
    "        global stream\n",
    "        print(\"TranslationRecognizerCallback open.\")\n",
    "        mic = pyaudio.PyAudio()\n",
    "        stream = mic.open(\n",
    "            format=pyaudio.paInt16, channels=1, rate=16000, input=True\n",
    "        )\n",
    "\n",
    "    def on_close(self) -> None:\n",
    "        global mic\n",
    "        global stream\n",
    "        print(\"TranslationRecognizerCallback close.\")\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        mic.terminate()\n",
    "        stream = None\n",
    "        mic = None\n",
    "\n",
    "    def on_event(\n",
    "        self,\n",
    "        request_id,\n",
    "        transcription_result: TranscriptionResult,\n",
    "        translation_result: TranslationResult,\n",
    "        usage,\n",
    "    ) -> None:\n",
    "        print(\"request id: \", request_id)\n",
    "        print(\"usage: \", usage)\n",
    "        if translation_result is not None:\n",
    "            print(\n",
    "                \"translation_languages: \",\n",
    "                translation_result.get_language_list(),\n",
    "            )\n",
    "            english_translation = translation_result.get_translation(\"en\")\n",
    "            print(\"sentence id: \", english_translation.sentence_id)\n",
    "            print(\"translate to english: \", english_translation.text)\n",
    "            if english_translation.stash is not None:\n",
    "                print(\n",
    "                    \"translate to english stash: \",\n",
    "                    translation_result.get_translation(\"en\").stash.text,\n",
    "                )\n",
    "        if transcription_result is not None:\n",
    "            print(\"sentence id: \", transcription_result.sentence_id)\n",
    "            print(\"transcription: \", transcription_result.text)\n",
    "            if transcription_result.stash is not None:\n",
    "                print(\"transcription stash: \", transcription_result.stash.text)\n",
    "\n",
    "\n",
    "callback = Callback()\n",
    "\n",
    "\n",
    "translator = TranslationRecognizerRealtime(\n",
    "    model=\"gummy-realtime-v1\",\n",
    "    format=\"pcm\",\n",
    "    sample_rate=16000,\n",
    "    transcription_enabled=True,\n",
    "    translation_enabled=True,\n",
    "    translation_target_languages=[\"en\"],\n",
    "    callback=callback,\n",
    ")\n",
    "translator.start()\n",
    "print(\"请您通过麦克风讲话体验实时语音识别和翻译功能\")\n",
    "while True:\n",
    "    if stream:\n",
    "        data = stream.read(3200, exception_on_overflow=False)\n",
    "        translator.send_audio_frame(data)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "translator.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 23:30:26.297564] websocket is open.\n",
      "[Metric] requestId: d427464109fd43d4a8879fa1aad50ddd, first package delay ms: -1741620626056.8052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-10 23:30:26.717828] audio result length: 1716\n",
      "[2025-03-10 23:30:26.717828] audio result length: 2926\n",
      "[2025-03-10 23:30:26.717828] audio result length: 2926\n",
      "[2025-03-10 23:30:26.741314] audio result length: 2926\n",
      "[2025-03-10 23:30:26.743954] audio result length: 2925\n",
      "[2025-03-10 23:30:26.743954] audio result length: 2926\n",
      "[2025-03-10 23:30:26.743954] audio result length: 2926\n",
      "[2025-03-10 23:30:26.743954] audio result length: 1671\n",
      "[2025-03-10 23:30:26.743954] audio result length: 2090\n",
      "[2025-03-10 23:30:26.751592] speech synthesis task complete successfully.\n",
      "[2025-03-10 23:30:26.751592] websocket is closed.\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import dashscope\n",
    "from dashscope.audio.tts_v2 import *\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp():\n",
    "    now = datetime.now()\n",
    "    formatted_timestamp = now.strftime(\"[%Y-%m-%d %H:%M:%S.%f]\")\n",
    "    return formatted_timestamp\n",
    "\n",
    "# 若没有将API Key配置到环境变量中，需将your-api-key替换为自己的API Key\n",
    "# dashscope.api_key = \"your-api-key\"\n",
    "\n",
    "model = \"cosyvoice-v1\"\n",
    "voice = \"longxiaochun\"\n",
    "\n",
    "\n",
    "class Callback(ResultCallback):\n",
    "    _player = None\n",
    "    _stream = None\n",
    "\n",
    "    def on_open(self):\n",
    "        self.file = open(\"output.mp3\", \"wb\")\n",
    "        print(get_timestamp() + \" websocket is open.\")\n",
    "\n",
    "    def on_complete(self):\n",
    "        print(get_timestamp() + \" speech synthesis task complete successfully.\")\n",
    "\n",
    "    def on_error(self, message: str):\n",
    "        print(f\"speech synthesis task failed, {message}\")\n",
    "\n",
    "    def on_close(self):\n",
    "        print(get_timestamp() + \" websocket is closed.\")\n",
    "        self.file.close()\n",
    "\n",
    "    def on_event(self, message):\n",
    "        pass\n",
    "\n",
    "    def on_data(self, data: bytes) -> None:\n",
    "        print(get_timestamp() + \" audio result length: \" + str(len(data)))\n",
    "        self.file.write(data)\n",
    "\n",
    "\n",
    "callback = Callback()\n",
    "\n",
    "synthesizer = SpeechSynthesizer(\n",
    "    model=model,\n",
    "    voice=voice,\n",
    "    callback=callback,\n",
    ")\n",
    "\n",
    "synthesizer.call(\"今天天气怎么样？\")\n",
    "print('[Metric] requestId: {}, first package delay ms: {}'.format(\n",
    "    synthesizer.get_last_request_id(),\n",
    "    synthesizer.get_first_package_delay()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
